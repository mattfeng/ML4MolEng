{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center> Problem Set 3 <center>\n",
    "<center> Spring 2021 <center>\n",
    "<center> 3.100/3.322, 10.402/10.602, 20.301/20.401 <center>\n",
    "<center> Due:10 pm ET on Thursday, Apr 1, 2021 <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Your name\n",
    "\n",
    "kerberos id: Your kerberos id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data \n",
    "! wget https://raw.githubusercontent.com/wwang2/ML4MolEng/master/psets/ps3/data/train_dna.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_pd = pd.read_csv(\"./train_dna.csv\")\n",
    "\n",
    "X = train_pd.seq.values\n",
    "y = train_pd.bind.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Understand an example deep learning workflow in PyTorch. Make sure you are comfortable with the example before proceeding to next parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Request a GPU on Google Colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if your GPU is requested successfully or not \n",
    "assert torch.cuda.device_count() != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build Datasets and DataLoaders in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode DNA sequence data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First you need to get comfortable with transforming array to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform your data (numpy arrays) to torch.Tensors \n",
    "\n",
    "myarray = np.ones(100)\n",
    "cpu_tensor = torch.Tensor(myarray)\n",
    "\n",
    "# Send tensor to GPU \n",
    "gpu_tensor = cpu_tensor.to('cuda:0') # send tensor to first cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn gpu_tensor to numpy arrays \n",
    "myarray = gpu_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your dataset class that takes in your X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Datasets\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Generate dataset \n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(X)  # store X as a pytorch Tensor\n",
    "        self.y = torch.Tensor(y)  # store y as a pytorch Tensor\n",
    "        self.len=len(self.X)                # number of samples in the data \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # your implementation here: \n",
    "        return torch.ones(100, 4), torch.ones(1,) # implement your __getitem__function here \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your Datasets and DataLoaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define dataset \n",
    "X_train = np.ones((1000, 100, 4))\n",
    "y_train = np.ones((1000, 1))\n",
    "X_val = np.ones((200, 100, 4))\n",
    "y_val = np.ones((200, 1))\n",
    "X_test = np.ones((200, 100, 4))\n",
    "y_test = np.ones((200, 1))\n",
    "\n",
    "#Build dataset \n",
    "traindata = SequenceDataset(X=X_train, y=y_train)\n",
    "valdata = SequenceDataset(X=X_val, y=y_val)\n",
    "testdata = SequenceDataset(X=X_test, y=y_test)\n",
    "\n",
    "# Build dataloader \n",
    "batchsize = 256\n",
    "train_loader = DataLoader(dataset=traindata,batch_size=batchsize,shuffle=True)\n",
    "val_loader = DataLoader(dataset=valdata,batch_size=batchsize,shuffle=True)\n",
    "test_loader = DataLoader(dataset=testdata,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop over your batches, and print the shape of a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, batch in enumerate(train_loader): \n",
    "    \n",
    "    # Your batch returns a X, y stacked in a batch \n",
    "    X_batch, y_batch = batch[0], batch[1]\n",
    "    if index == 0:\n",
    "        print(X_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of each batch? And how many batches are there in your dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the benefit of batching your data into mini-batches versus using the entire dataset to optimize the model all at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Build an LSTM-based binding classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First read documentations on torch.nn.LSTM, torch.nn.Sequential, torch.nn.Sigmoid and trying out the following lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torch.nn.LSTM module \n",
    "from torch import nn\n",
    "\n",
    "# Define a LSTM module \n",
    "lstm_model = nn.LSTM(input_size=4, hidden_size=16, batch_first=True).to(\"cuda:0\") # \"cuda:0\" is the device id\n",
    "\n",
    "# Send your batch to a GPU \n",
    "X_batch = X_batch.to(\"cuda:0\")\n",
    "y_batch = y_batch.to(\"cuda:0\")\n",
    "\n",
    "# Propagate your batch into your model \n",
    "lstm_out, (ht, ct) = lstm_model(X_batch) \n",
    "\n",
    "# You can play with hyperparameters to see how your output change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a MLP with a nn.Sequential that stacks nn.Linear and nn.ReLU()\n",
    "\n",
    "mlp = nn.Sequential( nn.Linear(16, 16), \n",
    "                     nn.ReLU(),  \n",
    "                     nn.Linear(16, 1)).to(\"cuda:0\")\n",
    "output = mlp(ht[-1])\n",
    "\n",
    "# define a sigmoid module that maps the scalar output to a probability \n",
    "sigmoid = nn.Sigmoid()\n",
    "proba = sigmoid(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross-Entropy to compute a loss function, i.e. your minimization target \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.binary_cross_entropy(proba.squeeze(), y_batch.squeeze()) # squeeze contruct the last dimension to make sure the dimensions match between y_batch and proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build your LSTM-based classifier as a nn.Module, you have to refine the modules inside a torch.nn.Module object.\n",
    "The main numerical is excuted in the forward() function. The foward() function should return a proba for each sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMseq(torch.nn.Module) :\n",
    "    def __init__(self, input_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        \n",
    "        # define a lstm module\n",
    "        # self.lstm = ... \n",
    "        \n",
    "        # define a mlp regressor \n",
    "        # self.mlp = ...\n",
    "        \n",
    "        # define a sigmod transform\n",
    "        # self.sigmoid = ...  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply lstm \n",
    "        \n",
    "        # pass ouput into a MLP \n",
    "        \n",
    "        # transform output into probabilites \n",
    "        \n",
    "        # return probabilities \n",
    "        \n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your ouput on a batch \n",
    "clf = LSTMseq(input_dim=4, hidden_dim=16).to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Implement functions for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    \n",
    "    '''\n",
    "    A function train on the entire dataset for one epoch .\n",
    "    \n",
    "    Args: \n",
    "        model (torch.nn.Module()): your sequence classifier \n",
    "        dataloader (torch.utils.data.Dataloader): Dataloader object for the train data\n",
    "        optimizer (torch.optim.Optimizer(()): optimizer object to interface gradient calculation and optimization \n",
    "        device (str): Your string\n",
    "        \n",
    "    Returns: \n",
    "        float: loss averaged over all the batches \n",
    "    \n",
    "    '''\n",
    "\n",
    "    batch_loss = []\n",
    "    model.train() # Set model to training mode \n",
    "    \n",
    "    for batch in dataloader:    \n",
    "        seq, label = batch\n",
    "        seq = seq.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # train your model on each batch here \n",
    "\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    \n",
    "    '''\n",
    "    A function validate on the validation dataset for one epoch .\n",
    "    \n",
    "    Args: \n",
    "        model (torch.nn.Module()): your sequence classifier \n",
    "        dataloader (torch.utils.data.Dataloader): Dataloader object for the train data\n",
    "        device (str): Your string\n",
    "        \n",
    "    Returns: \n",
    "        float: loss averaged over all the batches \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    val_loss = []\n",
    "    model.eval() # Set model to evaluation mode \n",
    "    with torch.no_grad():    \n",
    "        for batch in dataloader:\n",
    "            seq, label = batch\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # validate your model on each batch here \n",
    "            \n",
    "    return 0.0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "model = LSTMseq(4, 16).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"epoch\", \"train loss\", \"validation loss\")\n",
    "\n",
    "val_loss_curve = []\n",
    "train_loss_curve = []\n",
    "\n",
    "for epoch in range(500):\n",
    "    \n",
    "    # Compute train your model on training data\n",
    "    epoch_loss = train(model, train_loader, optimizer,  device=0)\n",
    "    \n",
    "    # Validate your on validation data \n",
    "    val_loss = validate(model, val_loader, device=0) \n",
    "    \n",
    "    # Record train and loss performance \n",
    "    train_loss_curve.append(epoch_loss)\n",
    "    val_loss_curve.append(val_loss)\n",
    "    \n",
    "    # The learning rate scheduler record the validation loss \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(epoch, epoch_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train and test curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(val_loss_curve)\n",
    "# plt.plot(train_loss_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report AUC score on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to compute AUC on test data \n",
    "\n",
    "    \n",
    "print(\"AUC on the tes dataset is {}\".format(test_score) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/wwang2/ML4MolEng/master/psets/ps3/data/drug.csv\n",
    "! wget https://raw.githubusercontent.com/wwang2/ML4MolEng/master/psets/ps3/data/morgan.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "df = pd.read_csv(\"drug.csv\")\n",
    "fp = np.loadtxt('morgan.csv', delimiter=',')\n",
    "\n",
    "assert fp.shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Principal Component Analysis on Molecular Fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform PCA to reduce data into vectors of 100 dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained variance ratio of the 100 principal components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"The first 100 components expalians {} of the total variance\".format(1.0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Patterns do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 T-SNE analysis on Molecular Fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform t-SNE anaylsis on the obtained principal components with three different perplexity values: 2, 50, 500, label your plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What differences do you see in the tree t-SNE plots? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Patterns do you observe for the active drugs for the perplexity=30 plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Are the low dimensional embeddings meaningful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in to 10 folds. For each fold, train on the other 9 folds and validate. Record your prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify your preidction into True Positives (TP), True Negatives (TN), FalsePositives (FP) and False Negatives (FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 2D t-sne embeddings (perplexity=30) colored by the four classification classes. (Your classifier might predict 0 False Postives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What pattern do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ps3]",
   "language": "python",
   "name": "conda-env-ps3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
